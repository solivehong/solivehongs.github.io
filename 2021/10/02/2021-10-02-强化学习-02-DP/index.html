<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>强化学习-动态规划-DP | Solivehong Blog</title><meta name="keywords" content="method"><meta name="author" content="Solivehong"><meta name="copyright" content="Solivehong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="强化学习-动态规划-DP 作者：YJLAugus  博客： https:&#x2F;&#x2F;www.cnblogs.com&#x2F;yjlaugus 项目地址：https:&#x2F;&#x2F;github.com&#x2F;YJLAugus&#x2F;Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。  这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习-动态规划-DP">
<meta property="og:url" content="https://zhaohongqiangsoliva.github.io/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/index.html">
<meta property="og:site_name" content="Solivehong Blog">
<meta property="og:description" content="强化学习-动态规划-DP 作者：YJLAugus  博客： https:&#x2F;&#x2F;www.cnblogs.com&#x2F;yjlaugus 项目地址：https:&#x2F;&#x2F;github.com&#x2F;YJLAugus&#x2F;Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。  这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://unpkg.com/justlovesmile-img/cover2.jpg">
<meta property="article:published_time" content="2021-10-01T16:00:00.000Z">
<meta property="article:modified_time" content="2022-10-27T13:28:55.046Z">
<meta property="article:author" content="Solivehong">
<meta property="article:tag" content="method">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://unpkg.com/justlovesmile-img/cover2.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zhaohongqiangsoliva.github.io/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '强化学习-动态规划-DP',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-27 21:28:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/dna.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://unpkg.com/justlovesmile-img/cover2.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Solivehong Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">强化学习-动态规划-DP</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-01T16:00:00.000Z" title="发表于 2021-10-02 00:00:00">2021-10-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-27T13:28:55.046Z" title="更新于 2022-10-27 21:28:55">2022-10-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="强化学习-动态规划-DP"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id=""><a href="#" class="headerlink" title=" "></a> </h1><h1 id="强化学习-动态规划-DP"><a href="#强化学习-动态规划-DP" class="headerlink" title="强化学习-动态规划-DP"></a>强化学习-动态规划-DP</h1><blockquote>
<p>作者：YJLAugus  博客： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a target="_blank" rel="noopener" href="https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star</a>。</p>
</blockquote>
<p>这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规划的方法，一个是<strong>策略迭代</strong>方法，另一个是<strong>价值迭代</strong>方法。</p>
<h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代主要分两个，一个是<strong>策略评估</strong>，一个是<strong>策略改进</strong>。定义比较简单，分别如下：</p>
<p><strong>策略评估</strong>：给定任意的一个 $\pi$ ，求出 状态价值函数 $v<em>\pi(s)$ 或者状态动作价值函数 $q</em>\pi(s,a)$。<br><strong>策略改进</strong>：依据策略评估得出来的 $v<em>\pi(s)$ 或者 $q</em>\pi(s,a)$，从其中构造出一个新的 $\pi’$ ，使得 $\pi’ \geq \pi $ ，这样就完成了一次迭代。</p>
<ul>
<li>换句话说，就是在每个状态下根据$v<em>\pi(s)$ 或者 $q</em>\pi(s,a)$选择一个最优的策略，这个策略就是 $\pi’$ 。</li>
<li>这种根据<code>原策略的价值函数</code>执行贪心算法，来构造一个更好策略的过程，我们称为策略改进。<br><strong>策略迭代</strong>： 就是递归以上两个过程，以上面的例子，得到$\pi’$的价值函数，然后再以$\pi’$的价值函数构造一个新的$\pi’’$ ，不断迭代。</li>
</ul>
<h3 id="策略评估-解析解"><a href="#策略评估-解析解" class="headerlink" title="策略评估-解析解"></a>策略评估-解析解</h3><p>策略评估，就是已知 MDP，“已知MDP”的意思是知道其动态特性，也就是 $p(s’,r \mid s,a)$，给定$\pi$ ，求$v_\pi(s)$。</p>
<p>已知$(\forall s\in S)$ ，即有多少个$s$ 就会有多少$v_\pi(s)$，可得列向量如下：</p>
<script type="math/tex; mode=display">
\large
v_\pi(s)= \begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})

\end{pmatrix} _{\lvert S \rvert \times 1}</script><p>上节课我们得到价值函数的公式，并进一步推导得到贝尔曼期望方程：</p>
<script type="math/tex; mode=display">
\large
{\begin{align}  
v_\pi(s) =& E_\pi[G_t \mid S_t = s] \\
=& E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1} \mid S_t=s)]\\
=& v_\pi(s) = \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p[s',r \mid s,a](r+ \gamma v_\pi(s'))
\end{align}
}</script><p>从上面的列向量中得知，如果我们想要求整个的$v<em>\pi(s)$ ，就是把$v</em>\pi(s<em>1),v</em>\pi(s<em>2),v</em>\pi(s_3)…$ 等所有的价值函数都要求出来。我们这节的目的就是把公式中的累加符号去掉，得到另一个更为简单的式子。接下来我们对下面的式子进行化简如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

v_\pi(s) =& \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p[s',r \mid s,a](r+ \gamma v_\pi(s'))\\
= & \underbrace {\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r }_{①}
+
\underbrace{\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s')}_{②}

\end{align}
}</script><p><strong>分解①式</strong>：对于①式而言，$s’$ 只出现在函数$p(s’,r \mid s,a)$ 中，是可以积分积掉的。故由①得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A}  \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\

\end{align}
}</script><p>（1）式正好是期望，此时我们也定义一个函数（记号） $r(s,a) \dot{=} E<em>\pi[R</em>{t+1}  \mid S_t=s,A_t=a]$ 。表示$s,a$ 这个二元组的收益。故我们的推导变成如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{r(s,a) \dot{=}E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\
=& \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2)

\end{align}
}</script><p>（2）式中，如果把 $a$ 都积分掉，那么（2）式就和$a$ 就没关系了，只和$s$ 有关，这里我们再次引入一个记号$r_\pi(s)$用来表示这个新的关系如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{r(s,a) \dot{=}E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\
=& \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2) \\
\dot{=}&r_\pi(s)  \quad\quad\quad\quad \quad\quad \quad\quad\quad \quad\quad\ \quad\quad \quad\quad\ (3)

\end{align}
}</script><p>此时，①式化简至此，需要提到的一点是，$r<em>\pi(s)$ 应该由多少个呢？在一开始我们就提到过 “$S$中有多少个$s$ 就会有多少$v</em>\pi(s)$，” ，显然 $r_\pi(s)$ 的数量也是 $\lvert S \rvert$ 个。故可得列向量如下:</p>
<script type="math/tex; mode=display">
\large
r_\pi(s)= \begin{pmatrix}
r_\pi(s_1)\\
r_\pi(s_2)\\
\vdots    \\
r_\pi(s_{\lvert S \rvert})

\end{pmatrix} _{\lvert S \rvert \times 1}</script><p><strong>分解②式</strong>：在 $p(s’,r \mid s,a) \cdot v_\pi(s’)$ 中，我们发现 $s’$ 在式子中都存在，但是$r$ 只存在于函数 $p(s’,r \mid s,a)$ 中，故积分可以积掉，则②式可以进一步写成：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s')
=& \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s'}\underbrace{p(s' \mid s,a)}_{状态转移函数} \cdot v_\pi(s') \quad\quad (4)

\end{align}
}</script><p>在（4）式中，可以发现在两个累加号中都有 $a$（注：虽然也都有 $s$，但是累加号下面是$a$ ，故积分只能积掉 $a$），积分可以积掉，但是一定和 $s,s’$ 有关，故引出一个记号 $P_\pi(s,s’)$ 来表示。 故继续推导如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s')
=& \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s'}\underbrace{p(s' \mid s,a)}_{状态转移函数} \cdot v_\pi(s') \quad\quad (4) \\

=& \gamma \sum_{s'} \underbrace{\sum_{a\in A} \pi(a\mid s) p(s' \mid s,a)}_{P_\pi(s,s')} \cdot v_\pi(s') \quad\quad (5) \\

=&  \gamma \sum_{s'} P_\pi(s,s') \cdot v_\pi(s') \quad\quad (6)\\

\end{align}
}</script><p>由（6）式可知，对于 $P<em>\pi(s,s’)$ 来说，一共由多少个值呢？很显然，$s$ 和 $s’$ 分别由 $\lvert S \rvert $ 个值，故 $P</em>\pi(s,s’)$ 会有 $\lvert S \rvert \times \lvert S \rvert  $  个。故可得矩阵如下：</p>
<script type="math/tex; mode=display">
\large
P_\pi(s,s')= \begin{pmatrix}
P_\pi(s_1,s'_1) & \cdots & P_\pi(s_1,s'_{\lvert S \rvert})\\
\vdots & \ddots & \vdots \\
P_\pi(s_{\lvert S \rvert},s'_1) & \cdots & P_\pi(s_{\lvert S \rvert},s'_{\lvert S \rvert})\\

\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}</script><p>于是，结合（3）和（6）式得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s,s') \cdot v_\pi(s') \quad\quad\quad (7)
}</script><p>令$s_i\dot{=s},s_j \dot{=} s’$ ，则（7） 式得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) = r_\pi(s) + \gamma \underbrace{\sum_{j=1}^{\lvert S \rvert} P_\pi(s_i,s_j) }_{①}\cdot v_\pi(s') \quad\quad\quad (7)
}</script><p>由（7）式中的①式可得，正好是矩阵$P<em>\pi(s,s’)$ ，其中的 $P</em>\pi(s<em>i,s_j)$正是矩阵的一行。（7）式中$v</em>\pi(s)，r<em>\pi(s)，①，v</em>\pi(s’)$ 式（$v<em>\pi(s’)$本质是和$v</em>\pi(s)$是一样的），都分别对应一个矩阵，也即：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})
\end{pmatrix}_{\lvert S \rvert \times 1}
=

\begin{pmatrix}
r_\pi(s_1)\\
r_\pi(s_2)\\
\vdots    \\
r_\pi(s_{\lvert S \rvert})
\end{pmatrix} _{\lvert S \rvert \times 1}
* \gamma

\begin{pmatrix}
P_\pi(s_1,s'_1) & \cdots & P_\pi(s_1,s'_{\lvert S \rvert})\\
\vdots & \ddots & \vdots \\
P_\pi(s_{\lvert S \rvert},s'_1) & \cdots & P_\pi(s_{\lvert S \rvert},s'_{\lvert S \rvert})\\
\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}
\cdot

\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})
\end{pmatrix} _{\lvert S \rvert \times 1}</script><p>最终化简到矩阵的运算。故，由（7）式进一步得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) =& r_\pi(s) + \gamma P_\pi(s,s') \cdot v_\pi(s') \quad\quad\quad \\
v_\pi=& r_\pi + \gamma P_\pi \cdot v_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad (8)\\
(I-P_\pi)v_\pi =& r_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad\quad\quad\quad\quad\ (9) \\
v_\pi =& (I-P_\pi)^{-1} r_\pi \qquad\qquad\qquad\qquad\qquad\qquad(10)
\end{align}
}</script><p>经过以上推导，得出（10）式。其中在（8）式中$v<em>\pi(s’)$ 其实是 $v</em>\pi(s)$ 的迭代，所以直接用 $v_\pi(s)$ 替换，在（9）式中$\gamma$ 是个常数，也可忽略。从向量中可以得出复杂度$O({\lvert S \rvert}^3)$</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203135011156.png" alt="image-20201203135011156"></p>
<h3 id="策略评估-迭代解"><a href="#策略评估-迭代解" class="headerlink" title="策略评估-迭代解"></a>策略评估-迭代解</h3><p>首先，我们来看如何使用动态规划来求解强化学习的<strong>预测问题</strong>，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从<strong>任意一个状态价值函数开始</strong>，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其<strong>收敛</strong>，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的<strong>状态价值</strong>计算出第$k+1$轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<script type="math/tex; mode=display">
\large{
v_{k+1}(s) = \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{k}(s'))
 \\

 v_{k+1}(s) \dot{=}\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p[s',r \mid s,a](r+ \gamma v_k(s'))
}</script><p>和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变<strong>很小(收敛)</strong>，那么我们就得出了预测问题的解，即给定策略的状态价值函数$v_\pi$。</p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203183436771.png" alt="image-20201203183436771"></p>
<h3 id="策略评估-（迭代解）应用"><a href="#策略评估-（迭代解）应用" class="headerlink" title="策略评估-（迭代解）应用"></a>策略评估-（迭代解）应用</h3><p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203142402216.png" alt="image-20201203142402216"></p>
<p>这是一个经典的Grid World的例子。我们有一个<code>4x4</code>的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为$\gamma =1$。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率$P =1$。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203142716568.png" alt="image-20201203142716568"></p>
<p>首先我们初始化所有格子的状态价值为0，如上图$k=0$的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。</p>
<p><strong>在$k=1$时</strong>，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">
v_1^{(21)} = \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] = -1</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">
v_1^{(22)} = \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] = -1</script><p>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图$k=1$的时候。现在我们第一轮迭代完了。</p>
<p><strong>在$k=1$时</strong>，还是看第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">
v_2^{(21)} = \frac{1}{4}[(-1+0) +(-1-1)+(-1-1)+(-1-1)] = -1.75</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">
v_2^{(22)} = \frac{1}{4}[(-1-1) +(-1-1)+(-1-1)+(-1-1)] = -2</script><p>最终得到的结果是上图$k=2$的时候，第二轮迭代完毕。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203142745214.png" alt="image-20201203142745214"></p>
<p><strong>在$k=3$时</strong>：</p>
<script type="math/tex; mode=display">
v_3^{(21)} = \frac{1}{4}[(-1+0)+(-1-1.7) +(-1-2)+(-1-2)] = -2.425 \\
v_3^{(22)} = \frac{1}{4}[(-1-1.7) +(-1-1.7)+(-1-2)+(-1-2)] = -2.85</script><blockquote>
<p>计算加和的过程 就是 上、下、左、右四个方向，其中无论哪次迭代，都有 $v_k^{11} = 0$。</p>
</blockquote>
<p>最终得到的结果是上图$k=3$的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小（收敛）为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h3 id="策略改进-策略改进定理"><a href="#策略改进-策略改进定理" class="headerlink" title="策略改进-策略改进定理"></a>策略改进-策略改进定理</h3><p>现在出现了这样一个问题，如果给定一个 $\pi$ 和 $\pi’$ ，我们如何判断哪一策略更好呢？采用我们以前提到的方法，就是分别计算各自对应的价值函数，然后通过判断两个价值函数的大小来判断策略的好坏。虽然能够得出结论，但是这个计算的过程也是会占用 “资源”的，能否有另外一种方式可以实现相应的功能呢？有，那就是<strong>策略改进定理。</strong></p>
<p><strong>策略改进定理：</strong> <strong>给定$\pi，\pi’$，如果$\forall(s) \in S,q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$，那么，则有 $\forall s\in S,v<em>{\pi’}(s) \geq v</em>\pi(s)$。</strong></p>
<p>通过上面的定理可得，$q<em>\pi(s,a)$ 只要算出来了，那么 $v</em>\pi(s)$ 自然也会得出（$v<em>\pi(s)$ 是$q</em>\pi(s,a)$的加权平均），此时我们不再需要再求得 $v<em>{\pi’}(s)$，而是直接把$\pi’(s)$ 带入到 $q</em>\pi(s,a)$ 中的a中即可。</p>
<p>下面进行定理的一个证明：</p>
<p>在 $q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$ 中，里面的 $q<em>\pi(s,a)$ 和 $v</em>\pi(s’)$ 有如下关系，我们在第一张MDP已经证明过：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
q_\pi(s,a) =& \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_\pi(s')) \\
=&E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t = s, A_t=a]
\end{align}
}</script><p>其实，这里需要说明一点，上式中 $E<em>\pi$ 中的$\pi$ 严格意义上是不能带着的，对于$R</em>{t+1}$ ， 不是$\pi$ 控制的，详情看下图的蓝色虚线（在第一章中，我们称其未系统之间的状态转移），</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/hss02.svg" alt="image-20201128135658979"></p>
<p>当然了，对于 $R<em>{t+2}, R</em>{t+3}$ 等是需要 $\pi$ 控制的，在公式中的体现就是$v_\pi$。故，这里准确写法应该如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
q_\pi(s,a) =& \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_\pi(s')) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=a]
\end{align}
}</script><p><strong>证明：</strong>$\forall(s) \in S,q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$，把 $a=\pi’(s)$ 带入得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad (2)

\end{align}
}</script><p>上面式子中，在（2）式中，我们把 $\pi’$那个策略定义到 $R<em>{t+1}$ 上，其余的还是采取 $\pi$ 的策略。在（2）式中出现了 $v</em>\pi(s<em>{t+1})$ ，再带入$v</em>\pi(s) \leq q_\pi(s,\pi’(s))$  得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3)

\end{align}
}</script><p>再把$q<em>\pi(s</em>{t+1},\pi’(s_{t+1}))$ 带入（2）式得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4)

\end{align}
}</script><p>有一点注意，在（4）式中 $S<em>{t+1}$ 没有写成等于多少的形式，这里只是对于$S</em>{t+2}$ 的前提条件，表明其不能独立出现。然后再对（4）展开得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5)

\end{align}
}</script><p>对于（5）式，出现了“期望套期望”，期望的期望还是期望，并且都是 $E{\pi’}$，故（5）式可以进一步化简得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5) \\

=&E_{\pi'}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t=s ]\quad\quad\ (6)

\end{align}
}</script><p>到这里，只走了两步，可以看到，由 $\pi’$ 一开始的只控制 $R<em>{t+1}$ ，走完两步后，$R</em>{t+2}$ 也属于 $\pi’$ 控制，接下来继续走的话，$\pi’$ 控制的 $R$ 会越来越多，$\pi$ 控制的 $R$ 越来越少。</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=& E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=& E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5) \\

=& E_{\pi'}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t=s ]\quad\quad\ (6)\\

\leq & E_{\pi'}[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^3v_\pi(S_{t+3}) \mid S_t=s ]\quad\quad\quad (7)\\

\vdots \\

\leq & E_{\pi'} \underbrace{[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^4 R_{t+4} + \cdots}_{G_t} \mid S_t=s ] \quad\quad\ (8)\\

=& v_{\pi'}(s)

\end{align}
}</script><p>故得到：$v<em>\pi’(s) \geq v</em>\pi(s)$， 得证。</p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201203210853298.png" alt="image-20201203210853298"></p>
<h3 id="策略改进-贪心策略"><a href="#策略改进-贪心策略" class="headerlink" title="策略改进-贪心策略"></a>策略改进-贪心策略</h3><p>这节就是利用策略改进定理提出一种策略改进的方法——<strong>贪心策略</strong>（Greedy Policy）。对于 $\forall s \in S$，定义如下公式：</p>
<script type="math/tex; mode=display">
\large{
\pi'(s) = \underset{a}{argmax}\ q_\pi(s,a) \quad\quad (1)
}</script><p>上面式子的意识是说，根据我们上节课所说的，从一个策略$\pi$，经过策略评估得到一些 $\pi’$ ，如下图所示。并从这些 $\pi’$ 中选择一个最大的$q_\pi(s,a)$。$\underset{a}{argmax}$ 表示能够使得表达式的值最大化的 $a$。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/pe.svg" alt=""></p>
<p>由在第一章讲到的 $v<em>\pi(s)$ 和 $q</em>\pi(s,a)$ 的 关系得：$v<em>\pi(s) \leq \underset{a}max \ q</em>\pi(s,a)$ 。又因由（1）式可得， $q<em>\pi(s,\pi’(s)) = \underset{a}max \ q</em>\pi(s,a)$ 故得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) \leq \underset{a}max \ q_\pi(s,a) =q_\pi(s,\pi'(s)) \\
v_\pi(s) \leq q_\pi(s,\pi'(s)) \quad\quad\quad \quad \qquad\quad\quad \qquad\quad\quad
(2)
}</script><p>（2）式正好满足上节我们说的策略改进定理。<strong>故由策略定理得知：对于 $\forall s \in S$，$v<em>{\pi’}(s) \geq v</em>\pi(s)$。</strong></p>
<p>如果在某一个时刻，一直迭代，如果出现了 $ v<em>\pi(s)=v</em>{\pi’}(s)$ ，这种情况，也就是说明 $v<em>\pi(s)$ 此时已经不能再好了，并且此时$ v</em>\pi(s)=v<em>{\pi’}(s) =v</em>*$。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/veq.svg" alt=""></p>
<blockquote>
<p><strong>证明</strong>：如果$v<em>{\pi’}=v</em>\pi$ 那么，$ v<em>\pi(s)=v</em>{\pi’}(s) =v_*$</p>
</blockquote>
<p>证：由 $v<em>{\pi’}=v</em>\pi$， 可以得出 $q<em>{\pi’}=q</em>\pi。$$\forall s\in S$，由$v<em>\pi(s) = \sum</em>{a\in A} \pi(a\mid s) ·q_\pi(s,a) $可得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\  

\end{align}
}</script><p>因为前面说的 $\pi’$ 都是选择一个最优的策略，也就是<strong>确定性策略</strong>，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/hss02.svg" alt="image-20201128135658979"></p>
<p>假如我们选择<code>a3</code>这个策略为 $\pi’$ 。故在（3）式中的加和可以去掉了，因为是确定性策略，那么选择<code>a2</code> 和 <code>a1</code>的概率就是0，例缩当然加和后只剩下<code>a3</code> 这个策略$\pi’$，故继续推导得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\ (4)  \\

\end{align}
}</script><p>由（4）式和（2）中$\underset{a}max \ q<em>\pi(s,a) =q</em>\pi(s,\pi’(s))$ 得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\

\end{align}
}</script><p>（5）式再由$q<em>\pi(s,a) =\sum</em>{s’,r}P<a href="r+ \gamma v_\pi(s&#39;">s’,r \mid s,a</a>) $ 可得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\
=& \underset{a}max \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_\pi(s')) \quad (6) \\
\end{align}
}</script><p>在（6）式中，又因为题设中，$v<em>{\pi’}=v</em>\pi$ 故带入得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\
=& \underset{a}max \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_\pi(s')) \quad (6) \\
=& \underset{a}max \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_{\pi’}(s')) \quad (7)
\end{align}
}</script><p>故此时我们得到：</p>
<script type="math/tex; mode=display">
\large
v_{\pi'}(s) = \underset{a}max \sum_{s',r}P[s',r \mid s,a](r+ \gamma v_{\pi’}(s')) \quad(8)</script><p>并且我们由贝尔曼最优方程得：</p>
<script type="math/tex; mode=display">
\large{
v_*(s)=\underset{a}{max}\sum_{s',r}P[s',r \mid s,a](r+\gamma v_\pi(s')) \quad\quad(9) \\

}</script><p>故由（8）式和（9）式，以及题设$v<em>{\pi’}=v</em>\pi$ 得证：</p>
<script type="math/tex; mode=display">
 \large
 v_\pi(s)=v_{\pi'}(s) =v_*</script><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201205145118572.png" alt="image-20201205145118572"></p>
<h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><h3 id="策略迭代的缺点"><a href="#策略迭代的缺点" class="headerlink" title="策略迭代的缺点"></a>策略迭代的缺点</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201205150228362.png" alt="image-20201205150228362"></p>
<p>从上图可以发现，在策略迭代中， 其实进行了两次策略循环，第一层是在策略评估中，第二层是策略迭代这一层。故，如果迭代次数过多的化，其实是很低效率的。<strong>策略评估的一个缺点是每一次迭代过程中都涉及了策略评估。</strong></p>
<h3 id="价值迭代-1"><a href="#价值迭代-1" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>根据以上， 这里我们讨论一种极端的情况，只计算第一次的价值函数 $v_1(s)$， 从此进行截断，后续不在计算，也即是说，只对 $v_1(s)$ 进行策略评估，并把这个算法称为<strong>价值迭代</strong>。如图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/jieduan1.svg" alt=""></p>
<p>如下图所示，价值迭代只计算 从状态$s$ 到$s’$的一条分支（$v<em>1(s)$），假定以最右侧分支为例，这是策略评估的<strong>一步</strong>，加上策略改进后，其实只走了“半步”，也就是从$s$ 状态走到a3（假定选择了action a3），此时利用策略改进算法即可用a3处的$q</em>\pi(s,a)$ ，并得到$v<em>\pi(s)$ 和 $v</em>{\pi’}(s)$的大小关系，以下图为例，假设a3为$v<em>\pi(s)$，则a2或者a1就是$v</em>{\pi’}(s)$。</p>
<p>也即：如果按照策略迭代的方法，需要计算完所有v1（选择a1的$v<em>\pi(s)-&gt;v</em>{\pi’}(s)$），v2，v3，如上图所示。但是依据价值迭代 + 策略改进，只需要计算v1的“半步”即可，因为根据策略改进定理，只需要计算出$q<em>\pi(s,a)$，即可得出$v</em>\pi(s)$ 和 $v_{\pi’}(s)$的大小关系。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/hss02.svg" alt="image-20201128135658979"></p>
<p>从 <strong>策略评估-（迭代解）应用</strong>小节 中可以知道，这个会有多个v1（注意：v1指的是多个状态，比如$v1^{21}$是第一次迭代中第二行第一个格子的状态，这里的“1”其实指的是迭代次数，并不一个状态）, 在这里选一个最大$v_\pi$作为$v<em>$ 。即完成了一次迭代。下面的公式，其是正是贝尔曼最优方程（求$v</em>$ ，$q<em>$）的 求$v</em>$的过程，同时也是价值迭代的算法公式：</p>
<script type="math/tex; mode=display">
\large{
 v_{k+1}(s) \dot{=}\underset{a}{max} \sum_{s',r}p(s',r \mid s,a)[r+ \gamma v_k(s') \quad\quad(1)
}</script><p>从上面（1）式中，从贝尔曼最优方程的较多来说，这里变成<strong>了一条更新规则</strong>，还是以<strong>策略评估-（迭代解）应用</strong>小节的例子举例，只计算v1，把“每个格子”的数据更新了一次（第一次迭代得到v1），而不需要在根据v1的数据j计算v2，再进行更新了。</p>
<p>除此之外，可以把（1）式和 “<strong>策略评估-迭代解</strong>”的更新策略$ v<em>{k+1}(s) \dot{=}\sum</em>{a\in A} \pi(a\mid s) \sum_{s’,r}p<a href="r+ \gamma v_k(s&#39;">s’,r \mid s,a</a>)$比较，唯一不同的是，在策略评估-迭代解中，对于 $s$状态的下一步<code>action</code> 需要根据概率求得，而在价值迭代中，直接在v1中选择最大的<code>action</code>，故概率此时为1。即：<strong>价值迭代是极端情况下的策略迭代</strong>。</p>
<p>根据上面描述的极端情况，其实就是价值迭代，总结如下：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201205213726436.png" alt="image-20201205213726436"></p>
<h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>异步动态规划，也叫就地迭代，它是基于价值迭代的一种算法。在样本空间比较大的情况下，即使只进行一次迭代，也会花费很长的时间，还是以<strong>策略评估-（迭代解）应用</strong> 小节的例子，比如下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201205220225280.png" alt="image-20201205220225280"></p>
<p>如果，格子的数量及其多，即使只计算v1，（更新一次），迭代一次，花费时间也是巨大的。所以就出现了一种算法，这种算法，只随机找到“其中的一个格子” 进行更新，这就是<strong>异步动态规划</strong>。然而，为了收敛，异步算法必须不断的更新所有状态的值。</p>
<h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>广义策略迭代，其实就包含了前面所说的一般的策略迭代，价值迭代，还有就地迭代。如下面的图，如果都走到“顶”，那就是一般的策略迭代。如果走不到“顶”就可能是其他的迭代。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/images/image-20201205221614255.png" alt="image-20201205221614255"></p>
<p>按照下面的例子和类比，就很容易理解了，可以把这个广义策略迭代类比买房：</p>
<p><strong>策略迭代</strong>：全款买房，以旧换新。</p>
<p><strong>价值迭代</strong>：首付，以旧换新。</p>
<p><strong>就地策略迭代</strong>：几乎0首付，以旧换新。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/9463815.html">https://www.cnblogs.com/pinard/p/9463815.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nV411k7ve?t=1738">https://www.bilibili.com/video/BV1nV411k7ve?t=1738</a></p>
<p><a target="_blank" rel="noopener" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/the-book.html">http://www.incompleteideas.net/book/the-book.html</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://zhaohongqiangsoliva.github.io">Solivehong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zhaohongqiangsoliva.github.io/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/">https://zhaohongqiangsoliva.github.io/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zhaohongqiangsoliva.github.io" target="_blank">Solivehong Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/method/">method</a></div><div class="post_share"><div class="social-share" data-image="https://unpkg.com/justlovesmile-img/cover2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/27/2022-10-27-linuxTools%E6%89%8B%E5%86%8C%E5%A4%A7%E5%85%A8/"><img class="prev-cover" src="https://unpkg.com/justlovesmile-img/cover3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">linux 命令大全</div></div></a></div><div class="next-post pull-right"><a href="/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/"><img class="next-cover" src="https://unpkg.com/justlovesmile-img/cover1.JPG" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">蒙特卡洛（Markov Chain &amp; Monte Carlo, MCMC）方法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/10/02/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-04-%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B/" title="强化学习—免模型预测"><img class="cover" src="https://unpkg.com/justlovesmile-img/cover5.JPG" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-02</div><div class="title">强化学习—免模型预测</div></div></a></div><div><a href="/2021/02/05/2022-09-04-Mixed_linear_model/" title="INTRODUCTION TO LINEAR MIXED MODELS"><img class="cover" src="https://unpkg.com/justlovesmile-img/cover2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-05</div><div class="title">INTRODUCTION TO LINEAR MIXED MODELS</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/dna.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Solivehong</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhaohongqiangsoliva" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zhaohongqiangsoliva@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text"> </span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-DP"><span class="toc-number">2.</span> <span class="toc-text">强化学习-动态规划-DP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.1.</span> <span class="toc-text">策略迭代</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-number">2.1.1.</span> <span class="toc-text">策略评估-解析解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">2.1.2.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%E8%BF%AD%E4%BB%A3%E8%A7%A3"><span class="toc-number">2.1.3.</span> <span class="toc-text">策略评估-迭代解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-1"><span class="toc-number">2.1.4.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%EF%BC%88%E8%BF%AD%E4%BB%A3%E8%A7%A3%EF%BC%89%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.5.</span> <span class="toc-text">策略评估-（迭代解）应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B-%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B%E5%AE%9A%E7%90%86"><span class="toc-number">2.1.6.</span> <span class="toc-text">策略改进-策略改进定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-2"><span class="toc-number">2.1.7.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B-%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5"><span class="toc-number">2.1.8.</span> <span class="toc-text">策略改进-贪心策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-3"><span class="toc-number">2.1.9.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.2.</span> <span class="toc-text">价值迭代</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">2.2.1.</span> <span class="toc-text">策略迭代的缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3-1"><span class="toc-number">2.2.2.</span> <span class="toc-text">价值迭代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="toc-number">2.2.3.</span> <span class="toc-text">异步动态规划</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">2.3.</span> <span class="toc-text">广义策略迭代</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">2.4.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/10/2023-10-30-owncloud%E6%90%AD%E5%BB%BA/" title="owncloud 搭建"><img src="https://unpkg.com/justlovesmile-img/cover6.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="owncloud 搭建"/></a><div class="content"><a class="title" href="/2024/01/10/2023-10-30-owncloud%E6%90%AD%E5%BB%BA/" title="owncloud 搭建">owncloud 搭建</a><time datetime="2024-01-09T16:00:00.000Z" title="发表于 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/10/2024-01-20-.megacli_RaidCard_tools/" title="owncloud 搭建"><img src="https://unpkg.com/justlovesmile-img/cover7.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="owncloud 搭建"/></a><div class="content"><a class="title" href="/2024/01/10/2024-01-20-.megacli_RaidCard_tools/" title="owncloud 搭建">owncloud 搭建</a><time datetime="2024-01-09T16:00:00.000Z" title="发表于 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/29/2023-10-29-%E7%BE%A4%E8%BE%89_arpl_esxi_6.7%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/" title="esxi 6.7 arpl 安装群辉的方法"><img src="https://unpkg.com/justlovesmile-img/cover6.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="esxi 6.7 arpl 安装群辉的方法"/></a><div class="content"><a class="title" href="/2023/10/29/2023-10-29-%E7%BE%A4%E8%BE%89_arpl_esxi_6.7%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/" title="esxi 6.7 arpl 安装群辉的方法">esxi 6.7 arpl 安装群辉的方法</a><time datetime="2023-10-28T16:00:00.000Z" title="发表于 2023-10-29 00:00:00">2023-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/27/2022-10-27-fzf%E7%9A%84%E4%BD%BF%E7%94%A8/" title="Fzf的使用"><img src="https://unpkg.com/justlovesmile-img/cover5.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Fzf的使用"/></a><div class="content"><a class="title" href="/2022/10/27/2022-10-27-fzf%E7%9A%84%E4%BD%BF%E7%94%A8/" title="Fzf的使用">Fzf的使用</a><time datetime="2022-10-26T16:00:00.000Z" title="发表于 2022-10-27 00:00:00">2022-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/27/2022-10-27-linuxTools%E6%89%8B%E5%86%8C%E5%A4%A7%E5%85%A8/" title="linux 命令大全"><img src="https://unpkg.com/justlovesmile-img/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="linux 命令大全"/></a><div class="content"><a class="title" href="/2022/10/27/2022-10-27-linuxTools%E6%89%8B%E5%86%8C%E5%A4%A7%E5%85%A8/" title="linux 命令大全">linux 命令大全</a><time datetime="2022-10-26T16:00:00.000Z" title="发表于 2022-10-27 00:00:00">2022-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Solivehong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>