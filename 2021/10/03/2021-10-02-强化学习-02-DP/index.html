
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title> - Hexo</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="强化学习-动态规划-DP
作者：YJLAugus  博客： https://www.cnblogs.com/yjlaugus 项目地址：https://github.com/YJLAugus/Rei,"> 
    <meta name="author" content="John Doe"> 
    <link rel="alternative" href="atom.xml" title="Hexo" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
    
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content=" - Hexo"/>
    <meta name="twitter:description" content="强化学习-动态规划-DP
作者：YJLAugus  博客： https://www.cnblogs.com/yjlaugus 项目地址：https://github.com/YJLAugus/Rei,"/>
    
    
    
    
    <meta property="og:site_name" content="Hexo"/>
    <meta property="og:type" content="object"/>
    <meta property="og:title" content=" - Hexo"/>
    <meta property="og:description" content="强化学习-动态规划-DP
作者：YJLAugus  博客： https://www.cnblogs.com/yjlaugus 项目地址：https://github.com/YJLAugus/Rei,"/>
    
<link rel="stylesheet" href="/css/diaspora.css">

    <script>window.searchDbPath = "/search.xml";</script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">
<meta name="generator" content="Hexo 6.2.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Hexo</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle"></h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title"></h1>
        <div class="stuff">
            <span>十月 03, 2021</span>
            

        </div>
        <div class="content markdown">
            <h1 id="强化学习-动态规划-DP"><a href="#强化学习-动态规划-DP" class="headerlink" title="强化学习-动态规划-DP"></a>强化学习-动态规划-DP</h1><blockquote>
<p>作者：YJLAugus  博客： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a target="_blank" rel="noopener" href="https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p>
</blockquote>
<p>这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规划的方法，一个是<strong>策略迭代</strong>方法，另一个是<strong>价值迭代</strong>方法。</p>
<h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代主要分两个，一个是<strong>策略评估</strong>，一个是<strong>策略改进</strong>。定义比较简单，分别如下：</p>
<p><strong>策略评估</strong>：给定任意的一个 $\pi$ ，求出 状态价值函数 $v<em>\pi(s)$ 或者状态动作价值函数 $q</em>\pi(s,a)$。<br><strong>策略改进</strong>：依据策略评估得出来的 $v<em>\pi(s)$ 或者 $q</em>\pi(s,a)$，从其中构造出一个新的 $\pi’$ ，使得 $\pi’ \geq \pi $ ，这样就完成了一次迭代。</p>
<ul>
<li>换句话说，就是在每个状态下根据$v<em>\pi(s)$ 或者 $q</em>\pi(s,a)$选择一个最优的策略，这个策略就是 $\pi’$ 。</li>
<li>这种根据<code>原策略的价值函数</code>执行贪心算法，来构造一个更好策略的过程，我们称为策略改进。<br><strong>策略迭代</strong>： 就是递归以上两个过程，以上面的例子，得到$\pi’$的价值函数，然后再以$\pi’$的价值函数构造一个新的$\pi’’$ ，不断迭代。</li>
</ul>
<h3 id="策略评估-解析解"><a href="#策略评估-解析解" class="headerlink" title="策略评估-解析解"></a>策略评估-解析解</h3><p>策略评估，就是已知 MDP，“已知MDP”的意思是知道其动态特性，也就是 $p(s’,r \mid s,a)$，给定$\pi$ ，求$v_\pi(s)$。</p>
<p>已知$(\forall s\in S)$ ，即有多少个$s$ 就会有多少$v_\pi(s)$，可得列向量如下：</p>
<script type="math/tex; mode=display">
\large
v_\pi(s)= \begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})

\end{pmatrix} _{\lvert S \rvert \times 1}</script><p>上节课我们得到价值函数的公式，并进一步推导得到贝尔曼期望方程：</p>
<script type="math/tex; mode=display">
\large
{\begin{align}  
v_\pi(s) =& E_\pi[G_t \mid S_t = s] \\
=& E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1} \mid S_t=s)]\\
=& v_\pi(s) = \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a)[r+ \gamma v_\pi(s')] 
\end{align}
}</script><p>从上面的列向量中得知，如果我们想要求整个的$v<em>\pi(s)$ ，就是把$v</em>\pi(s<em>1),v</em>\pi(s<em>2),v</em>\pi(s_3)…$ 等所有的价值函数都要求出来。我们这节的目的就是把公式中的累加符号去掉，得到另一个更为简单的式子。接下来我们对下面的式子进行化简如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align} 

v_\pi(s) =& \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a)[r+ \gamma v_\pi(s')]\\
= & \underbrace {\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r }_{①}
+ 
\underbrace{\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s')}_{②}


\end{align}
}</script><p><strong>分解①式</strong>：对于①式而言，$s’$ 只出现在函数$p(s’,r \mid s,a)$ 中，是可以积分积掉的。故由①得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A}  \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\


\end{align}
}</script><p>（1）式正好是期望，此时我们也定义一个函数（记号） $r(s,a) \dot{=} E<em>\pi[R</em>{t+1}  \mid S_t=s,A_t=a]$ 。表示$s,a$ 这个二元组的收益。故我们的推导变成如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{r(s,a) \dot{=}E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\
=& \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2)



\end{align}
}</script><p>（2）式中，如果把 $a$ 都积分掉，那么（2）式就和$a$ 就没关系了，只和$s$ 有关，这里我们再次引入一个记号$r_\pi(s)$用来表示这个新的关系如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot r  \\
=& \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}_{r(s,a) \dot{=}E_\pi[R_{t+1}  \mid S_t=s,A_t=a]} \quad\quad (1)  \\
=& \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2) \\
\dot{=}&r_\pi(s)  \quad\quad\quad\quad \quad\quad \quad\quad\quad \quad\quad\ \quad\quad \quad\quad\ (3)



\end{align}
}</script><p>此时，①式化简至此，需要提到的一点是，$r<em>\pi(s)$ 应该由多少个呢？在一开始我们就提到过 “$S$中有多少个$s$ 就会有多少$v</em>\pi(s)$，” ，显然 $r_\pi(s)$ 的数量也是 $\lvert S \rvert$ 个。故可得列向量如下:</p>
<script type="math/tex; mode=display">
\large
r_\pi(s)= \begin{pmatrix}
r_\pi(s_1)\\
r_\pi(s_2)\\
\vdots    \\
r_\pi(s_{\lvert S \rvert})

\end{pmatrix} _{\lvert S \rvert \times 1}</script><p><strong>分解②式</strong>：在 $p(s’,r \mid s,a) \cdot v_\pi(s’)$ 中，我们发现 $s’$ 在式子中都存在，但是$r$ 只存在于函数 $p(s’,r \mid s,a)$ 中，故积分可以积掉，则②式可以进一步写成：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s') 
=& \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s'}\underbrace{p(s' \mid s,a)}_{状态转移函数} \cdot v_\pi(s') \quad\quad (4)


\end{align}
}</script><p>在（4）式中，可以发现在两个累加号中都有 $a$（注：虽然也都有 $s$，但是累加号下面是$a$ ，故积分只能积掉 $a$），积分可以积掉，但是一定和 $s,s’$ 有关，故引出一个记号 $P_\pi(s,s’)$ 来表示。 故继续推导如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}

\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a) \cdot v_\pi(s') 
=& \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s'}\underbrace{p(s' \mid s,a)}_{状态转移函数} \cdot v_\pi(s') \quad\quad (4) \\


=& \gamma \sum_{s'} \underbrace{\sum_{a\in A} \pi(a\mid s) p(s' \mid s,a)}_{P_\pi(s,s')} \cdot v_\pi(s') \quad\quad (5) \\

=&  \gamma \sum_{s'} P_\pi(s,s') \cdot v_\pi(s') \quad\quad (6)\\


\end{align}
}</script><p>由（6）式可知，对于 $P<em>\pi(s,s’)$ 来说，一共由多少个值呢？很显然，$s$ 和 $s’$ 分别由 $\lvert S \rvert $ 个值，故 $P</em>\pi(s,s’)$ 会有 $\lvert S \rvert \times \lvert S \rvert  $  个。故可得矩阵如下：</p>
<script type="math/tex; mode=display">
\large
P_\pi(s,s')= \begin{pmatrix}
P_\pi(s_1,s'_1) & \cdots & P_\pi(s_1,s'_{\lvert S \rvert})\\
\vdots & \ddots & \vdots \\
P_\pi(s_{\lvert S \rvert},s'_1) & \cdots & P_\pi(s_{\lvert S \rvert},s'_{\lvert S \rvert})\\

\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}</script><p>于是，结合（3）和（6）式得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s,s') \cdot v_\pi(s') \quad\quad\quad (7)
}</script><p>令$s_i\dot{=s},s_j \dot{=} s’$ ，则（7） 式得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) = r_\pi(s) + \gamma \underbrace{\sum_{j=1}^{\lvert S \rvert} P_\pi(s_i,s_j) }_{①}\cdot v_\pi(s') \quad\quad\quad (7)
}</script><p>由（7）式中的①式可得，正好是矩阵$P<em>\pi(s,s’)$ ，其中的 $P</em>\pi(s<em>i,s_j)$正是矩阵的一行。（7）式中$v</em>\pi(s)，r<em>\pi(s)，①，v</em>\pi(s’)$ 式（$v<em>\pi(s’)$本质是和$v</em>\pi(s)$是一样的），都分别对应一个矩阵，也即：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})
\end{pmatrix} _{\lvert S \rvert \times 1}
=

\begin{pmatrix}
r_\pi(s_1)\\
r_\pi(s_2)\\
\vdots    \\
r_\pi(s_{\lvert S \rvert})
\end{pmatrix} _{\lvert S \rvert \times 1}
+ \gamma

\begin{pmatrix}
P_\pi(s_1,s'_1) & \cdots & P_\pi(s_1,s'_{\lvert S \rvert})\\
\vdots & \ddots & \vdots \\
P_\pi(s_{\lvert S \rvert},s'_1) & \cdots & P_\pi(s_{\lvert S \rvert},s'_{\lvert S \rvert})\\
\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}
\cdot 

\begin{pmatrix}
v_\pi(s_1)\\
v_\pi(s_2)\\
\vdots    \\
v_\pi(s_{\lvert S \rvert})
\end{pmatrix} _{\lvert S \rvert \times 1}</script><p>最终化简到矩阵的运算。故，由（7）式进一步得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) =& r_\pi(s) + \gamma P_\pi(s,s') \cdot v_\pi(s') \quad\quad\quad \\
v_\pi=& r_\pi + \gamma P_\pi \cdot v_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad (8)\\
(I-P_\pi)v_\pi =& r_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad\quad\quad\quad\quad\ (9) \\
v_\pi =& (I-P_\pi)^{-1} r_\pi \qquad\qquad\qquad\qquad\qquad\qquad(10)
\end{align}
}</script><p>经过以上推导，得出（10）式。其中在（8）式中$v<em>\pi(s’)$ 其实是 $v</em>\pi(s)$ 的迭代，所以直接用 $v_\pi(s)$ 替换，在（9）式中$\gamma$ 是个常数，也可忽略。从向量中可以得出复杂度$O({\lvert S \rvert}^3)$</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203135011156.png" alt="image-20201203135011156"></p>
<h3 id="策略评估-迭代解"><a href="#策略评估-迭代解" class="headerlink" title="策略评估-迭代解"></a>策略评估-迭代解</h3><p>首先，我们来看如何使用动态规划来求解强化学习的<strong>预测问题</strong>，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从<strong>任意一个状态价值函数开始</strong>，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其<strong>收敛</strong>，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的<strong>状态价值</strong>计算出第$k+1$轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<script type="math/tex; mode=display">
\large{
v_{k+1}(s) = \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_{k}(s'))
 \\

 v_{k+1}(s) \dot{=}\sum_{a\in A} \pi(a\mid s) \sum_{s',r}p(s',r \mid s,a)[r+ \gamma v_k(s')]
}</script><p>和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变<strong>很小(收敛)</strong>，那么我们就得出了预测问题的解，即给定策略的状态价值函数$v_\pi$。</p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203183436771.png" alt="image-20201203183436771"></p>
<h3 id="策略评估-（迭代解）应用"><a href="#策略评估-（迭代解）应用" class="headerlink" title="策略评估-（迭代解）应用"></a>策略评估-（迭代解）应用</h3><p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142402216.png" alt="image-20201203142402216"></p>
<p>这是一个经典的Grid World的例子。我们有一个<code>4x4</code>的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为$\gamma =1$。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率$P =1$。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142716568.png" alt="image-20201203142716568"></p>
<p>首先我们初始化所有格子的状态价值为0，如上图$k=0$的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。</p>
<p><strong>在$k=1$时</strong>，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">
v_1^{(21)} = \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] = -1</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">
v_1^{(22)} = \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] = -1</script><p>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图$k=1$的时候。现在我们第一轮迭代完了。</p>
<p><strong>在$k=1$时</strong>，还是看第二行第一个格子的价值：</p>
<script type="math/tex; mode=display">
v_2^{(21)} = \frac{1}{4}[(-1+0) +(-1-1)+(-1-1)+(-1-1)] = -1.75</script><p>第二行第二个格子的价值是：</p>
<script type="math/tex; mode=display">
v_2^{(22)} = \frac{1}{4}[(-1-1) +(-1-1)+(-1-1)+(-1-1)] = -2</script><p>最终得到的结果是上图$k=2$的时候，第二轮迭代完毕。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142745214.png" alt="image-20201203142745214"></p>
<p><strong>在$k=3$时</strong>：</p>
<script type="math/tex; mode=display">
v_3^{(21)} = \frac{1}{4}[(-1+0)+(-1-1.7) +(-1-2)+(-1-2)] = -2.425 \\
v_3^{(22)} = \frac{1}{4}[(-1-1.7) +(-1-1.7)+(-1-2)+(-1-2)] = -2.85</script><blockquote>
<p>计算加和的过程 就是 上、下、左、右四个方向，其中无论哪次迭代，都有 $v_k^{11} = 0$。</p>
</blockquote>
<p>最终得到的结果是上图$k=3$的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小（收敛）为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h3 id="策略改进-策略改进定理"><a href="#策略改进-策略改进定理" class="headerlink" title="策略改进-策略改进定理"></a>策略改进-策略改进定理</h3><p>现在出现了这样一个问题，如果给定一个 $\pi$ 和 $\pi’$ ，我们如何判断哪一策略更好呢？采用我们以前提到的方法，就是分别计算各自对应的价值函数，然后通过判断两个价值函数的大小来判断策略的好坏。虽然能够得出结论，但是这个计算的过程也是会占用 “资源”的，能否有另外一种方式可以实现相应的功能呢？有，那就是<strong>策略改进定理。</strong></p>
<p><strong>策略改进定理：</strong> <strong>给定$\pi，\pi’$，如果$\forall(s) \in S,q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$，那么，则有 $\forall s\in S,v<em>{\pi’}(s) \geq v</em>\pi(s)$。</strong></p>
<p>通过上面的定理可得，$q<em>\pi(s,a)$ 只要算出来了，那么 $v</em>\pi(s)$ 自然也会得出（$v<em>\pi(s)$ 是$q</em>\pi(s,a)$的加权平均），此时我们不再需要再求得 $v<em>{\pi’}(s)$，而是直接把$\pi’(s)$ 带入到 $q</em>\pi(s,a)$ 中的a中即可。</p>
<p>下面进行定理的一个证明：</p>
<p>在 $q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$ 中，里面的 $q<em>\pi(s,a)$ 和 $v</em>\pi(s’)$ 有如下关系，我们在第一张MDP已经证明过：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
q_\pi(s,a) =& \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_\pi(s')] \\
=&E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t = s, A_t=a]
\end{align}
}</script><p>其实，这里需要说明一点，上式中 $E<em>\pi$ 中的$\pi$ 严格意义上是不能带着的，对于$R</em>{t+1}$ ， 不是$\pi$ 控制的，详情看下图的蓝色虚线（在第一章中，我们称其未系统之间的状态转移），</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>当然了，对于 $R<em>{t+2}, R</em>{t+3}$ 等是需要 $\pi$ 控制的，在公式中的体现就是$v_\pi$。故，这里准确写法应该如下：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
q_\pi(s,a) =& \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_\pi(s')] \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=a]
\end{align}
}</script><p><strong>证明：</strong>$\forall(s) \in S,q<em>\pi(s,\pi’(s)) \geq v</em>\pi(s)$，把 $a=\pi’(s)$ 带入得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad (2)


\end{align}
}</script><p>上面式子中，在（2）式中，我们把 $\pi’$那个策略定义到 $R<em>{t+1}$ 上，其余的还是采取 $\pi$ 的策略。在（2）式中出现了 $v</em>\pi(s<em>{t+1})$ ，再带入$v</em>\pi(s) \leq q_\pi(s,\pi’(s))$  得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3)


\end{align}
}</script><p>再把$q<em>\pi(s</em>{t+1},\pi’(s_{t+1}))$ 带入（2）式得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4)


\end{align}
}</script><p>有一点注意，在（4）式中 $S<em>{t+1}$ 没有写成等于多少的形式，这里只是对于$S</em>{t+2}$ 的前提条件，表明其不能独立出现。然后再对（4）展开得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5)


\end{align}
}</script><p>对于（5）式，出现了“期望套期望”，期望的期望还是期望，并且都是 $E{\pi’}$，故（5）式可以进一步化简得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=&E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5) \\

=&E_{\pi'}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t=s ]\quad\quad\ (6)


\end{align}
}</script><p>到这里，只走了两步，可以看到，由 $\pi’$ 一开始的只控制 $R<em>{t+1}$ ，走完两步后，$R</em>{t+2}$ 也属于 $\pi’$ 控制，接下来继续走的话，$\pi’$ 控制的 $R$ 会越来越多，$\pi$ 控制的 $R$ 越来越少。</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_\pi(s) \leq & q_\pi(s,\pi'(s)) \\
=& E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t=\pi'(s)] \quad\quad\quad(1) \\
=& E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \\

\leq & E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) \mid S_t = s] \quad\quad\quad\quad\quad\ (3) \\

=& E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (4) \\

=& E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi'}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t=s ]\quad\quad\ (5) \\

=& E_{\pi'}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t=s ]\quad\quad\ (6)\\

\leq & E_{\pi'}[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^3v_\pi(S_{t+3}) \mid S_t=s ]\quad\quad\quad (7)\\

\vdots \\

\leq & E_{\pi'} \underbrace{[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^4 R_{t+4} + \cdots}_{G_t} \mid S_t=s ] \quad\quad\ (8)\\ 

=& v_{\pi'}(s)

\end{align}
}</script><p>故得到：$v<em>\pi’(s) \geq v</em>\pi(s)$， 得证。</p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203210853298.png" alt="image-20201203210853298"></p>
<h3 id="策略改进-贪心策略"><a href="#策略改进-贪心策略" class="headerlink" title="策略改进-贪心策略"></a>策略改进-贪心策略</h3><p>这节就是利用策略改进定理提出一种策略改进的方法——<strong>贪心策略</strong>（Greedy Policy）。对于 $\forall s \in S$，定义如下公式：</p>
<script type="math/tex; mode=display">
\large{
\pi'(s) = \underset{a}{argmax}\ q_\pi(s,a) \quad\quad (1)
}</script><p>上面式子的意识是说，根据我们上节课所说的，从一个策略$\pi$，经过策略评估得到一些 $\pi’$ ，如下图所示。并从这些 $\pi’$ 中选择一个最大的$q_\pi(s,a)$。$\underset{a}{argmax}$ 表示能够使得表达式的值最大化的 $a$。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/pe.svg" alt=""></p>
<p>由在第一章讲到的 $v<em>\pi(s)$ 和 $q</em>\pi(s,a)$ 的 关系得：$v<em>\pi(s) \leq \underset{a}max \ q</em>\pi(s,a)$ 。又因由（1）式可得， $q<em>\pi(s,\pi’(s)) = \underset{a}max \ q</em>\pi(s,a)$ 故得：</p>
<script type="math/tex; mode=display">
\large{
v_\pi(s) \leq \underset{a}max \ q_\pi(s,a) =q_\pi(s,\pi'(s)) \\
v_\pi(s) \leq q_\pi(s,\pi'(s)) \quad\quad\quad \quad \qquad\quad\quad \qquad\quad\quad
(2)
}</script><p>（2）式正好满足上节我们说的策略改进定理。<strong>故由策略定理得知：对于 $\forall s \in S$，$v<em>{\pi’}(s) \geq v</em>\pi(s)$。</strong></p>
<p>如果在某一个时刻，一直迭代，如果出现了 $ v<em>\pi(s)=v</em>{\pi’}(s)$ ，这种情况，也就是说明 $v<em>\pi(s)$ 此时已经不能再好了，并且此时$ v</em>\pi(s)=v<em>{\pi’}(s) =v</em>*$。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/veq.svg" alt=""></p>
<blockquote>
<p><strong>证明</strong>：如果$v<em>{\pi’}=v</em>\pi$ 那么，$ v<em>\pi(s)=v</em>{\pi’}(s) =v_*$</p>
</blockquote>
<p>证：由 $v<em>{\pi’}=v</em>\pi$， 可以得出 $q<em>{\pi’}=q</em>\pi。$$\forall s\in S$，由$v<em>\pi(s) = \sum</em>{a\in A} \pi(a\mid s) ·q_\pi(s,a) $可得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\  

\end{align}
}</script><p>因为前面说的 $\pi’$ 都是选择一个最优的策略，也就是<strong>确定性策略</strong>，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>假如我们选择<code>a3</code>这个策略为 $\pi’$ 。故在（3）式中的加和可以去掉了，因为是确定性策略，那么选择<code>a2</code> 和 <code>a1</code>的概率就是0，例缩当然加和后只剩下<code>a3</code> 这个策略$\pi’$，故继续推导得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\ (4)  \\

\end{align}
}</script><p>由（4）式和（2）中$\underset{a}max \ q<em>\pi(s,a) =q</em>\pi(s,\pi’(s))$ 得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\

\end{align}
}</script><p>（5）式再由$q<em>\pi(s,a) =\sum</em>{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] $ 可得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\
=& \underset{a}max \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_\pi(s')] \quad (6) \\
\end{align}
}</script><p>在（6）式中，又因为题设中，$v<em>{\pi’}=v</em>\pi$ 故带入得：</p>
<script type="math/tex; mode=display">
\large{
\begin{align}
v_{\pi'}(s) =& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi'}(s,a)\\
=& \sum_{a\in A} {\pi'}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \\
=& q_\pi(s,\pi'(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \\
=& \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\\
=& \underset{a}max \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_\pi(s')] \quad (6) \\
=& \underset{a}max \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_{\pi’}(s')] \quad (7)
\end{align}
}</script><p>故此时我们得到：</p>
<script type="math/tex; mode=display">
\large
v_{\pi'}(s) = \underset{a}max \sum_{s',r}P(s',r \mid s,a)[r+ \gamma v_{\pi’}(s')] \quad(8)</script><p>并且我们由贝尔曼最优方程得：</p>
<script type="math/tex; mode=display">
\large{
v_*(s)=\underset{a}{max}\sum_{s',r}P(s',r \mid s,a)[r+\gamma v_\pi(s')] \quad\quad(9) \\


}</script><p>故由（8）式和（9）式，以及题设$v<em>{\pi’}=v</em>\pi$ 得证：</p>
<script type="math/tex; mode=display">
 \large
 v_\pi(s)=v_{\pi'}(s) =v_*</script><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205145118572.png" alt="image-20201205145118572"></p>
<h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><h3 id="策略迭代的缺点"><a href="#策略迭代的缺点" class="headerlink" title="策略迭代的缺点"></a>策略迭代的缺点</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205150228362.png" alt="image-20201205150228362"></p>
<p>从上图可以发现，在策略迭代中， 其实进行了两次策略循环，第一层是在策略评估中，第二层是策略迭代这一层。故，如果迭代次数过多的化，其实是很低效率的。<strong>策略评估的一个缺点是每一次迭代过程中都涉及了策略评估。</strong></p>
<h3 id="价值迭代-1"><a href="#价值迭代-1" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>根据以上， 这里我们讨论一种极端的情况，只计算第一次的价值函数 $v_1(s)$， 从此进行截断，后续不在计算，也即是说，只对 $v_1(s)$ 进行策略评估，并把这个算法称为<strong>价值迭代</strong>。如图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/jieduan1.svg" alt=""></p>
<p>如下图所示，价值迭代只计算 从状态$s$ 到$s’$的一条分支（$v<em>1(s)$），假定以最右侧分支为例，这是策略评估的<strong>一步</strong>，加上策略改进后，其实只走了“半步”，也就是从$s$ 状态走到a3（假定选择了action a3），此时利用策略改进算法即可用a3处的$q</em>\pi(s,a)$ ，并得到$v<em>\pi(s)$ 和 $v</em>{\pi’}(s)$的大小关系，以下图为例，假设a3为$v<em>\pi(s)$，则a2或者a1就是$v</em>{\pi’}(s)$。</p>
<p>也即：如果按照策略迭代的方法，需要计算完所有v1（选择a1的$v<em>\pi(s)-&gt;v</em>{\pi’}(s)$），v2，v3，如上图所示。但是依据价值迭代 + 策略改进，只需要计算v1的“半步”即可，因为根据策略改进定理，只需要计算出$q<em>\pi(s,a)$，即可得出$v</em>\pi(s)$ 和 $v_{\pi’}(s)$的大小关系。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>从 <strong>策略评估-（迭代解）应用</strong>小节 中可以知道，这个会有多个v1（注意：v1指的是多个状态，比如$v1^{21}$是第一次迭代中第二行第一个格子的状态，这里的“1”其实指的是迭代次数，并不一个状态）, 在这里选一个最大$v_\pi$作为$v<em>$ 。即完成了一次迭代。下面的公式，其是正是贝尔曼最优方程（求$v</em>$ ，$q<em>$）的 求$v</em>$的过程，同时也是价值迭代的算法公式：</p>
<script type="math/tex; mode=display">
\large{
 v_{k+1}(s) \dot{=}\underset{a}{max} \sum_{s',r}p(s',r \mid s,a)[r+ \gamma v_k(s') \quad\quad(1)
}</script><p>从上面（1）式中，从贝尔曼最优方程的较多来说，这里变成<strong>了一条更新规则</strong>，还是以<strong>策略评估-（迭代解）应用</strong>小节的例子举例，只计算v1，把“每个格子”的数据更新了一次（第一次迭代得到v1），而不需要在根据v1的数据j计算v2，再进行更新了。</p>
<p>除此之外，可以把（1）式和 “<strong>策略评估-迭代解</strong>”的更新策略$ v<em>{k+1}(s) \dot{=}\sum</em>{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’)]$比较，唯一不同的是，在策略评估-迭代解中，对于 $s$状态的下一步<code>action</code> 需要根据概率求得，而在价值迭代中，直接在v1中选择最大的<code>action</code>，故概率此时为1。即：<strong>价值迭代是极端情况下的策略迭代</strong>。</p>
<p>根据上面描述的极端情况，其实就是价值迭代，总结如下：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205213726436.png" alt="image-20201205213726436"></p>
<h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>异步动态规划，也叫就地迭代，它是基于价值迭代的一种算法。在样本空间比较大的情况下，即使只进行一次迭代，也会花费很长的时间，还是以<strong>策略评估-（迭代解）应用</strong> 小节的例子，比如下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205220225280.png" alt="image-20201205220225280"></p>
<p>如果，格子的数量及其多，即使只计算v1，（更新一次），迭代一次，花费时间也是巨大的。所以就出现了一种算法，这种算法，只随机找到“其中的一个格子” 进行更新，这就是<strong>异步动态规划</strong>。然而，为了收敛，异步算法必须不断的更新所有状态的值。</p>
<h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>广义策略迭代，其实就包含了前面所说的一般的策略迭代，价值迭代，还有就地迭代。如下面的图，如果都走到“顶”，那就是一般的策略迭代。如果走不到“顶”就可能是其他的迭代。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205221614255.png" alt="image-20201205221614255"></p>
<p>按照下面的例子和类比，就很容易理解了，可以把这个广义策略迭代类比买房：</p>
<p><strong>策略迭代</strong>：全款买房，以旧换新。</p>
<p><strong>价值迭代</strong>：首付，以旧换新。</p>
<p><strong>就地策略迭代</strong>：几乎0首付，以旧换新。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/9463815.html">https://www.cnblogs.com/pinard/p/9463815.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nV411k7ve?t=1738">https://www.bilibili.com/video/BV1nV411k7ve?t=1738</a></p>
<p><a target="_blank" rel="noopener" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/the-book.html">http://www.incompleteideas.net/book/the-book.html</a></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title="0" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                        
                            <li title="1" data-url="http://link.hhtjim.com/163/425570952.mp3"></li>
                        
                    
                </ul>
            
        </div>
        
        
    <div id="gitalk-container" class="comment link"
		data-enable="false"
        data-ae="false"
        data-ci=""
        data-cs=""
        data-r=""
        data-o=""
        data-a=""
        data-d="false"
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




</html>
